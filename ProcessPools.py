'''
This file contains the Manager Classes which handle the parts of Update process:
extracting from cab files, updating the database and searching for symbols
'''
#***************************************
# Imports
#***************************************
import asyncio
import queue
import os
import sqlite3
from time import time
import threading
import subprocess
import traceback as tb
import logging, logging.handlers
import hashlib
from concurrent.futures import ProcessPoolExecutor, wait
from pathlib import Path
from typing import Tuple
import re

from defusedxml import ElementTree as et
import pefile

from db import wsuse_db
import globs
from support.utils import validatecab, ispe, validatezip, \
    getfilehashes, ispebuiltwithdebug, pebinarytype, getpepdbfilename, \
    getpeage, getpearch, getpesigwoage, ispedbgstripped, rmfile
import MSDelta_imp


#****************************************************
# Local Variables
#****************************************************
_mgrlogger = logging.getLogger("BAM.Pools")

def mgr_logconfig(queue: queue.Queue):
    global _mgrlogger

    qh = logging.handlers.QueueHandler(queue)
    _mgrlogger.addHandler(qh)
    _mgrlogger.setLevel(logging.INFO)

def wkr_logconfig(queue: queue.Queue, logger: logging.Logger):
    parent = logger
    qh = logging.handlers.QueueHandler(queue)
    fh = logging.Formatter("[%(process)d][Thread %(thread)d] %(message)s")
    qh.setFormatter(fh)
    parent.addHandler(qh)
    parent.setLevel(logging.INFO)

#****************************************************
# Classes
#****************************************************
class CabMgr(threading.Thread):
    '''
    pdir - directory to cab files to be extracted
    dest - directory to place extracted results
    poolsize - number of processes to spawn and keep track of
    pemgr - PEMgr to pass results to
    db - DBMgr for which to refer database write jobs to
    '''
    def __init__(self, pdir: str, dest: str, poolsize: int, pemgr, psfxmgr, db, local: bool, globqueue: queue.Queue):
        super(CabMgr, self).__init__()
        self.pdir = pdir
        self.dest = dest
        self.poolsize = poolsize
        self.pemgr = pemgr
        self.psfxmgr = psfxmgr
        self.dbc = db
        # jobs - a list of cab files that have not been distributed to worker processes
        self.jobs = queue.Queue()
        # workRemaining - the amount of work that is not complete, which is not strictly the
        # same as the length of the jobs list, since work remaining includes jobs taken off
        # of the list to be worked on by processes
        self.workremaining = 0
        # jobsincoming - an event to signal to the ProcessPoolExecutor that more jobs have
        # been added to the jobs list
        self.jobsincoming = threading.Event()
        self.userinterrupt = threading.Event()
        self.localaction = local
        self.globqueue = globqueue
        self.cabmgrlogger = logging.getLogger("BAM.Pools.CabMgr")
         # self.daemon = True

    def addq(self, taskpath: str):
        '''
        adds item to job list from which the manager will task workers with
        '''
        self.jobs.put(taskpath)
        self.jobsincoming.set()
        self.workremaining += 1

    def passresult(self, future: asyncio.Future):
        '''
        takes future generated by executor process and passes result to
        pemgr or psfxmgr
        '''
        # if exception generated by extracttask, notify here. If not, get result and send to
        # pemgr
        fexception = future.exception()
        if fexception is KeyboardInterrupt:
            self.cabmgrlogger.log(logging.INFO, "[CABMGR] execution stopped by user")
        elif fexception is not None:
            self.cabmgrlogger.log(logging.ERROR, "[CABMGR] {-} exception occurred: " + str(fexception) + \
                "\n\ttraceback: " + tb.format_exc())
        else:
            job = future.result()
            if job is not None:
                # if the directory contains ncabdir, then we
                # know that it is already a nestedCab, so remove
                # it from deliverables, otherwise, send to DBMgr
                if job[0] == "PSFX":
                    version = int(job[6])
                    for j in job[2]:
                        self.psfxmgr.addjob((j, version, job[3]))
                    for j in job[1]:
                        self.pemgr.addjob(j, job[3])
                elif job[0] == "nonPSFX":
                    for j in job[1]:
                        self.pemgr.addjob(j, job[3])
                    self.cabmgrlogger.log(logging.INFO, "[CABMGR] sent to pemgr: " + str(job[0][0]))
                else:
                    self.cabmgrlogger.log(logging.INFO, "[CABMGR] no jobs passed to pemgr")
            else:
                self.cabmgrlogger.log(logging.INFO, "[CABMGR] job did not contain patched binaries or additional updates")

        # if there are no more nested cabs, set the jobs incoming event since there is at least 1 more job item to allow thread to
        # complete execution
        self.workremaining -= 1
        self.cabmgrlogger.log(logging.INFO, "[CABMGR] work remaining: " + str(self.workremaining))
        if self.workremaining == 0:
            self.jobsincoming.set()

    def queueDbTask(self, future: asyncio.Future):
        '''
        takes future generated by executor process and queues job for dbmanager, if applicable
        '''
        fexception = future.exception()
        if fexception is not None:
            self.cabmgrlogger.log(logging.ERROR, "[CABMGR] {-} exception occurred: " + str(fexception) + \
                "\n\ttraceback: " + tb.format_exc())
        else:
            job = future.result()
            if job is not None:
                jobtuple = (job[5], job[0])
                sha256 = job[3]
                sha1 = job[4]
                self.dbc.addtask("update", jobtuple, sha256, sha1, None)

    def run(self):
        '''
        CabMgr finds all cab files in target directory and adds to jobslist.
        Then assigns jobslist items to worker processes which handle extraction/DB update.
        Finally receives results back from workers, requeues nested cabs if any,
        and passes results to pemgr.
        '''
        start = time()
        self.cabmgrlogger.log(logging.INFO, "[CABMGR] starting CABMGR")
        if self.pemgr is not None:
            self.pemgr.cabstartsig()
        if self.psfxmgr is not None:
            self.psfxmgr.cabstartsig()
        # search for and add cab files to task queue
        for root, _, files, in os.walk(self.pdir):
            for file in files:
                filel = file.lower()
                if filel.endswith(".cab") or filel.endswith(".msu") \
                    or filel.endswith(".exe"):
                    self.addq(os.path.join(root, file))
        
        global _mgrlogger
        with ProcessPoolExecutor(max_workers=self.poolsize, initializer=wkr_logconfig, initargs=(self.globqueue, _mgrlogger)) as executor:
            while not self.jobs.empty():
                if self.userinterrupt.is_set():
                    self.cabmgrlogger.log(logging.INFO, "[CABMGR] execution stopped by user")
                    break
                self.cabmgrlogger.log(logging.INFO, "[CABMGR] assigning extraction job")
                if self.localaction:
                    future = executor.submit(self.dbupdate, self.jobs.get(), \
                                                self.dest)
                else:
                    future = executor.submit(self.extracttask, self.jobs.get(), \
                                                self.dest)
                    future.add_done_callback(self.passresult)
                
                future.add_done_callback(self.queueDbTask)               
        
        if self.pemgr:
            self.pemgr.cabdonesig()
            self.cabmgrlogger.log(logging.INFO, "[CABMGR] done signal sent to pemgr")
        else:
            self.cabmgrlogger.log(logging.INFO, "[CABMGR] pemgr doesn't exist because working in update only mode")
        if self.psfxmgr:
            self.psfxmgr.cabdonesig()
            self.cabmgrlogger.log(logging.INFO, "[CABMGR] done signal sent to psfxmgr")
        else:
            self.cabmgrlogger.log(logging.INFO, "[CABMGR] psfxmgr doesn't exist because working in update only mode")

        self.dbc.donesig()

        end = time()
        elapsed = end-start
        print("**********************************part 1 done*************************************")
        print("elapsed time for part 1: " + str(elapsed))

    @staticmethod
    def verifyentry(src: str, sha256: str, sha1: str, logger: logging.Logger) -> bool:
        '''
        verify DB entry
        '''
        logmsg = "[CABMGR] Verifying entry for " + src
        logger.log(logging.INFO, logmsg)

        filepath = None

        try:
            filepath = str(Path(src).resolve())
        except OSError as error:
            logmsg = "[CABMGR] Issue getting full path for " + src + ": " + str(error) + ". Using unresolved path."
            logger.log(logging.ERROR, logmsg)
            filepath = src

        if wsuse_db.dbentryexist(globs.DBCONN.cursor(),     \
                                globs.UPDATEFILESDBNAME, sha256, sha1):
            logmsg = "[CABMGR] {-} item " + filepath + " already exists in db, skipping"
            logger.log(logging.WARNING, logmsg)
            return True
        return False

    @staticmethod
    def performcablisting(src: str, logger: logging.Logger) -> str|None:
        '''
        Call expand.exe to get a listing of files within a CAB/MSU
        '''
        result = None
        logmsg = "[CABMGR] Listing " + str(src) + " CAB contents"
        logger.log(logging.INFO, logmsg)

        filepath = os.environ['systemdrive'] + "\\Windows\\system32\\expand.exe"

        if not os.path.isfile(filepath):
            logmsg = "[CABMGR] {-} expand.exe was not found. Given path: " + filepath
            logger.log(logging.ERROR, logmsg)
            return None

        args = filepath + " -D \"" + str(src) + "\""

        try:
            with subprocess.Popen(args, shell=False, stdout=subprocess.PIPE) as pexp:
                intermediate, _ = pexp.communicate()
                result = intermediate.decode("ascii")
        except subprocess.CalledProcessError as error:
            logmsg = "[CABMGR] {-} Listing contents of " + src + \
                   " failed with " + str(error.returncode) + " " +  \
                   str(error.stderr)
            logger.log(logging.ERROR, logmsg)
            result = None
        except FileNotFoundError as error:
            logmsg = ("[CABMGR] {-} expand.exe not found")
            logger.log(logging.ERROR, logmsg)
            result = None

        return result

    @staticmethod
    def performcabextract(extstr: str, src: str, newdir: str, logger: logging.Logger) -> str|None:
        '''
        Call expand.exe to extract files (if any)
        https://support.microsoft.com/en-us/help/928636/you-cannot-extract-the-contents-of-a-microsoft-update-standalone-packa
        https://blogs.msdn.microsoft.com/astebner/2008/03/11/knowledge-base-article-describing-how-to-extract-msu-files-and-automate-installing-them/
        '''
        result = None
        args = os.environ['systemdrive'] + "\\Windows\\system32\\expand -R \"" + str(src) + "\" -F:" + str(extstr) + " \"" + str(newdir) + "\""
        try:
            with subprocess.Popen(args, shell=False, stdout=subprocess.PIPE) as pexp:
                rawstdout, _ = pexp.communicate()
                result = rawstdout.decode("ascii")
            logmsg = "[CABMGR] extracted " + extstr + " at " + newdir
            logger.log(logging.INFO, logmsg)
        except subprocess.CalledProcessError as error:
            logmsg = "[CABMGR] {-} extracting " + extstr + " from " + src +    \
                    " failed with " + str(error.returncode) + " " +  \
                    error.output.decode('ascii') + ".\n\n" + \
                    "{-} cmd (" + str(error.cmd) + ") stderr (" + \
                    str(error.stderr) + ")"
            logger.log(logging.ERROR, logmsg)
            result = None
        except FileNotFoundError as error:
            logmsg = ("[CABMGR] {-} expand.exe not found")
            logger.log(logging.ERROR, logmsg)
            result = None

        return result

    @staticmethod
    def perform7zextract(src: str, newpath: str, logger: logging.Logger):
        '''
        Call 7z.exe to extract files (if any)
        '''
        # stdout will be the std output of the 7zip subprocess
        stdout = None
        logmsg = "[CABMGR] Performing 7z on " + newpath
        logger.log(logging.INFO, logmsg)
        filepath = os.environ["PROGRAMW6432"] + "\\7-Zip\\7z.exe"
        
        if not os.path.isfile(filepath):
            logmsg = "[CABMGR] {-} 7z.exe was not found. Given path: " + filepath
            logger.log(logging.ERROR, logmsg)
            return None

        args =  filepath + " x -aoa -o\"" + str(newpath) + "\" -y -r \"" +str(src) + "\" *.dll *.sys *.exe"

        try:
            with subprocess.Popen(args, shell=False, stdout=subprocess.PIPE) as p7z:
                stdout, _ = p7z.communicate()
        except subprocess.CalledProcessError as error:
            logmsg = "[CABMGR] {-} extracting, using 7z, from " + src + \
                     " failed with " + str(error.returncode) + " " +  \
                     error.output.decode('ascii')
            logger.log(logging.ERROR, logmsg)
            stdout = None
        except FileNotFoundError as error:
            logmsg = ("[PBSK] {-} 7z.exe not found")
            logger.log(logging.ERROR, logmsg)
            stdout = None
        
        return stdout

    @classmethod
    def dbupdate(cls, src: str, pdestdir: str) -> Tuple | None:
        '''
        when given a directory of updates (CABs/MSUs/ZIPs)
        and no extraction, only set up update files to be added to dbc.
        Destination is where patched files are.
        '''
        cablogger = logging.getLogger("BAM.Pools.cabwkr")
        
        logmsg = "[CABMGR][DBUP] starting on " + str(src)
        cablogger.log(logging.INFO, logmsg)

        # initialize deliverables
        deliverables = None
        newpath = ''

        hashes = getfilehashes(src)

        if hashes is None:
            return None

        if not (validatecab(src) or ispe(src) or validatezip(src)):
            logmsg = "[CABMGR][DBUP] invalid cab/pe/zip"
            cablogger.log(logging.ERROR, logmsg)
            return None

        newname = src.split("\\")[-1].lstrip()
        newpath = pdestdir + "\\" + newname

        if ".exe" in newname:
            newpath = newpath.split(".exe")[0]
        elif ".cab" in newname:
            newpath = newpath.split(".cab")[0]
        elif ".zip" in newname:
            newpath = newpath.split(".zip")[0]

        deliverables = ("", [newpath], None, hashes[0], hashes[1], newname, 0)
        # No need to locate nested CABs/MSUs as long the parent update file
        # is found. Revisit if needed

        logmsg = "[CABMGR][DBUP] Extraction (DB update only) task completed for " + src
        cablogger.log(logging.INFO, logmsg)

        # Send the job to the next manager (DB will be updated eventually)
        return deliverables

    @classmethod
    def extracttask(cls, src: str, dst: str) -> Tuple | None:
        '''task for workers to extract contents of update files and return directory of
        results to PEMgr. Also gathers metadata and hands that information off to DBMgr
        to update the database with. When encountering a PSFX style update, will instead pass the 
        update to PSFXMgr for processing, and will allow PSFXMgr to update the db rather
        than itself.'''
        cablogger = logging.getLogger("BAM.Pools.cabwkr")
        DBupdateName = src.split("\\")[-1]

        # first check if information already exists in db
        hashes = getfilehashes(src)
        if hashes is None:
            raise Exception("Unable to calculate Hashes for update.")
        if cls.verifyentry(src, hashes[0], hashes[1], cablogger):
            return None

        # now check if src is an executable which will be extracted using 7zip
        if ispe(src):
            logmsg = "[CABMGR] extracting PE file (" + src + ")..."
            cablogger.log(logging.INFO, logmsg)
            newname = src.split("\\")[-1].lstrip()
            newdir = (dst + "\\" + newname).split(".exe")[0]
            try:
                os.mkdir(newdir)
            except FileExistsError:
                pass
            except OSError as oserror:
                logmsg = "[CABMGR] OSError creating new directory... skipping extraction for (" + \
                    src + "). Error: " + str(oserror)
                cablogger.log(logging.ERROR, logmsg)
                return None

            if cls.perform7zextract(src, newdir, cablogger) is None:
                # if nothing was extracted, remove the directory to clean up
                try:
                    os.rmdir(newdir)
                except OSError:
                    pass
                return None

            return ("nonPSFX", [newdir], None, hashes[0], hashes[1], DBupdateName, 0)
        # assuming that Microsoft is not devious enough to put a cab in a exe
        # proceed to extract cab/msu contents
        cabfile = src
        extractdir = dst + "\\" + src.split("\\")[-1][0:-4]
        try:
            os.mkdir(extractdir)
        except FileExistsError:
            pass

        # perform breadth first extraction
        cabqueue = queue.LifoQueue()
        cabqueue.put(cabfile) # preload queue so loop will work
        psfxlist = []
        nonpsfxlist = []
        packageFormat = "undetermined"
        version = 0

        while not cabqueue.empty():
            cabfile = cabqueue.get()
            if not cabfile == src:
                extractdir = "\\".join(cabfile.split("\\")[0:-1])

            listing = cls.performcablisting(cabfile, cablogger)
            
            # found that mum files in psfx cabs will have a psfx package format
            # attribute in the xml.
            if "update.mum" in listing:
                cls.performcabextract("update.mum", cabfile, extractdir, cablogger)
                mum = extractdir + "\\update.mum"
                version = cls.getVersion(mum)
                packageFormat = cls.getPackageFormat(mum)

            windows_update_psf = re.search(r"[wW]indows1\d\.0-[kK][bB]\d{7}-[\da-zA-Z]{3,5}\.psf", listing)
            if windows_update_psf:
                windows_update_cab_match = re.search(r"[wW]indows1\d\.0-[kK][bB]\d{7}-[\da-zA-Z]{3,5}\.cab", listing)
                windows_update_cab = extractdir + "\\" + windows_update_cab_match[0]
                windows_update_cab_dir = windows_update_cab[:-4]

                try:
                    os.mkdir(windows_update_cab_dir)
                    windows_update_cab = windows_update_cab_dir + "\\" + windows_update_cab_match[0]
                except FileExistsError:
                    continue

                cls.performcabextract(windows_update_psf[0], cabfile, windows_update_cab_dir, cablogger)
                cls.performcabextract(windows_update_cab_match[0], cabfile, windows_update_cab_dir, cablogger)

                cls.performcabextract("update.mum", windows_update_cab, windows_update_cab_dir, cablogger)
                cls.performcabextract("express.psf.cix.xml", windows_update_cab, windows_update_cab_dir, cablogger)

                mum = windows_update_cab_dir + "\\update.mum"
                version = cls.getVersion(mum)
                packageFormat = cls.getPackageFormat(mum)

                psfx_dir = windows_update_cab_dir + "\\" + windows_update_psf[0]
                psfxlist.append(psfx_dir)

            # presumably, at the point where this becomes true, we've already handled the contents that we actully want out of this cab, so we should remove it and not process it.
            if "express.psf.cix.xml" in listing:
                os.remove(cabfile)
                continue

            if not cabfile == src:
                if version >= 17763 and packageFormat == "PSFX":
                    psfxlist.append(cabfile)
                else:
                    nonpsfxlist.append(cabfile)

            lines = listing.split("\r\n")
            for line in lines:
                # if a line contains a PE file, then we've reached the end of a branch,
                # so break and go to the next cab.
                if ".dll" in line or ".exe" in line or ".sys" in line:
                    break
                
                itemincab = line.split(" ")[-1].rstrip("\r\n\t ")
                if (itemincab.endswith(".cab") or itemincab.endswith(".msu")) and "WSUSSCAN" not in itemincab:
                    cabdir = extractdir + "\\" + itemincab[0:-4]
                    nextcab = cabdir + "\\" + itemincab
                    
                    cabqueue.put(nextcab)
                    try:
                        os.mkdir(cabdir)
                    except FileExistsError:
                        pass

                    cls.performcabextract(itemincab, cabfile, cabdir, cablogger)
            
        # for non psfx items, extract those contents immediately
        for cab in nonpsfxlist:
            cabdst = "\\".join(cab.split("\\")[0:-1])
            try:
                cls.performcabextract("*.dll", cab, cabdst, cablogger)
                cls.performcabextract("*.exe", cab, cabdst, cablogger)
                cls.performcabextract("*.sys", cab, cabdst, cablogger)

                os.remove(cab)
            except FileNotFoundError:
                logmsg = "cab file not found"
                cablogger.log(logging.ERROR, logmsg)

        if len(psfxlist) < 1:
            return ("nonPSFX", nonpsfxlist, None, hashes[0], hashes[1], DBupdateName, version)
        else:
            return ("PSFX", nonpsfxlist, psfxlist, hashes[0], hashes[1], DBupdateName, version)

    @classmethod
    def getPackageFormat(cls, xmlfile: str) -> str:
        '''read an xml file to find the packageformat of an update'''
        xml = et.parse(xmlfile)
        root = xml.getroot()
        for tag in root.iter('{urn:schemas-microsoft-com:asm.v3}customInformation'):
            if 'PackageFormat' in tag.attrib:
                # this will return the top level instance of version
                return tag.attrib['PackageFormat']
        return None

    @classmethod
    def getVersion(cls, xmlfile: str) -> int:
        '''read the update.mum file to find the version of an update'''
        # find version like format: 10.0.22621.8
        version_style1 = re.compile(r"(\d{1,2}).[01].(\d{5}).(\d+)")
        # find version like format: 22621.1532.2.6
        version_style2 = re.compile(r"(\d{5}).(\d{4}).(\d+).(\d+)")

        with open(xmlfile, 'r') as file:
            xml = file.read()
            match = version_style1.search(xml)
            if not match:
                match = version_style2.search(xml)
                if not match:
                    raise Exception("Version not found")
                version = match[0].split(".")[0]
                return int(version)
            version = match[0].split(".")[2]
            return int(version)


class PSFXMgr(CabMgr):
    '''
    separate manager to handle psfx cabs files that cab manager doesn't have the tools to do.
    jobs will be queue of cab files
    '''
    def __init__(self, pdir: str, dest: str, poolsize: int, pemgr, db, local: bool, globqueue: queue.Queue, basedir: str):
        super(PSFXMgr, self).__init__(pdir, dest, poolsize, pemgr, self, db, local, globqueue)
        self.jobs = queue.Queue()
        self.cabmgrRunning = threading.Event()
        self.userinterrupt = threading.Event()
        self.psfxmgrlogger = logging.getLogger("BAM.Pools.PSFXMgr")

        # assumes that all base files needed are dumped into a single directory.
        index = {}
        version_regex = re.compile(r"\d{1,2}\.[01]\.\d{5}\.\d+")
        product_regex = re.compile(r"[^\s\\]+_[\da-f]{16}_\d{1,2}\.[01]\.\d{5}\.\d+_\S+_[\da-f]{16}")
        chaff_regex = re.compile(r"_[\da-f]{16}")
        for root, _, files in os.walk(basedir):
            for file in files:
                full_path = os.path.join(root, file)
                full_product = product_regex.search(full_path)
                if full_product:
                    version_match = version_regex.search(full_path)
                    product_match = chaff_regex.split(full_product[0])[0]

                    search_terms = (int(version_match[0].split(".")[2]), product_match, file)
                    index.update({search_terms: full_path})
        self.index = index

    
    def run(self):
        start = time()
        self.cabmgrRunning.wait()
        self.pemgr.psfxstartsig()
        
        global _mgrlogger
        futures = []
        with ProcessPoolExecutor(max_workers=self.poolsize, initializer=wkr_logconfig, initargs=(self.globqueue, _mgrlogger)) as executor:
            while self.cabmgrRunning.is_set() or not self.jobs.empty():
                if self.userinterrupt.is_set():
                    self.psfxmgrlogger.log(logging.INFO, "[PSFXMGR] execution stopped by user")
                    break
                try:
                    item = self.jobs.get(block=True, timeout=60)
                    tuple = (item[0], item[1])
                    UpdateSha = item[2]
                except queue.Empty:
                    # this is done because we know that we'll get instances of queue being empty
                    # but since we may still want to wait for other items to come in while the 
                    # previous manager is running, we need to ignore the queue being empty.
                    # Previously we used the regular get which blocked for waiting for another 
                    # item, but this ended up causing the thread to hang because the loop could
                    # not check the event that said the previous manager had completed.
                    continue
                psfxcab = tuple[0]
                version = int(tuple[1])
                dest = "\\".join(psfxcab.split("\\")[0:-1])
                future = executor.submit(self.PSFXExtract, psfxcab, dest, self.index, version, UpdateSha)
                future.add_done_callback(self.passresult)
                futures.append(future)
            wait(futures)
                 
        try:
            for future in futures:
                print(future.result())
        except Exception as e:
            print(str(e))

        self.pemgr.psfxdonesig()
        self.psfxmgrlogger.log(logging.INFO, "[PSFXMGR] done signal sent to pemgr")
        end = time()
        elapsed = end - start
        print("*********************PSFX extraction done**********************")
        print("[PSFXMGR] time taken for psfx extract: ", elapsed)
        
    @classmethod
    def findBaseFile(cls, index: dict, product_name: str, version: int, file: str) -> str:
        # all paths returned should be full paths
        # basepath: the directory where every version of base file can be found
        # product_name: what I assume is the overall product that the file we're searching for is a part of
        # version: the Windows build version.
        # potential to be able to search through the WinSxS dir of every version of windows to be able to find base files?
        
        # Assuming that something like 10.0.22621.1 is the base file for 22621.
        search_terms = (version, product_name, file)
        if search_terms in index.keys():
            return index[search_terms]
        return "Base version " + str(version) + " not found\n"

    @classmethod
    def PSFXExtract(cls, src: str, dest: str, index: dict, version: int, UpdateSha: str) -> Tuple:
        # procesing code
        psfxwkrlog = logging.getLogger("BAM.Pools.psfxwkr")
        full_product_regex = re.compile(r"([^\s\\])+_[\da-f]{16}_\d{1,2}\.[01]\.\d{5}\.\d+_\S+_[\da-f]{16}")
        chaff_regex = re.compile(r"_[\da-f]{16}")
        # windows_update_regex = re.compile(r"windows1\d\.0-kb\d{7}-[\da-zA-Z]{3,5}")

        # starting with 22000, they started packaging the delta files into a '.psf' file blob and also removed the reverse deltas
        if version >= 22000:
            # get the patch file and the index cab out from the overall cab
            # updatename = windows_update_regex.search(src)
            # if not updatename:
            #     logmsg = "update name for " + src + " not found for psf extraction"
            #     psfxwkrlog.log(logging.ERROR, logmsg)
            #     return dest
            # psfName = updatename[0] + ".psf"
            # psfPath = dest + "\\" + psfName

            # use express.psf.cix.xml to carve out all the relevant PE files from the psf
            with open(src, 'rb') as file:
                psfContents = file.read()
                
                # get file name, length, offset, and filetime from here
                xmlfiledir = dest + "\\express.psf.cix.xml"
                xml = et.parse(xmlfiledir)
                root = xml.getroot()
                tag = "{urn:ContainerIndex}File"
                for leaf in root.iter(tag):
                    DeltaNameParts = leaf.attrib['name'].split("\\")
                    # if the item isn't a PE file, then we don't care about it and want to skip it
                    if not (DeltaNameParts[-1].endswith(".dll") or DeltaNameParts[-1].endswith(".exe") or DeltaNameParts[-1].endswith(".sys")):
                        continue
                    # PeTime = leaf.attrib['time']
                    DeltaLen = int(leaf[1][0].attrib['length'])
                    DeltaOffset = int(leaf[1][0].attrib['offset'])

                    # get the specific bytes from the psf file and write them to a new forward delta file
                    DeltaBytes = psfContents[DeltaOffset:DeltaOffset+DeltaLen]
                    DeltaDirExten = "\\".join(DeltaNameParts[:-1])
                    DeltaFileName = DeltaNameParts[-1]
                    forward_delta_dir = dest + "\\" + DeltaDirExten

                    if not os.path.exists(forward_delta_dir):
                        os.makedirs(forward_delta_dir)

                    forward_delta = forward_delta_dir + "\\" + DeltaFileName
                    with open(forward_delta, 'wb') as out:
                        out.write(DeltaBytes)
                    
                    # use the forward delta to hydrate the full PE
                    product_name = chaff_regex.split(DeltaNameParts[0])[0]
                    base = cls.findBaseFile(index, product_name, version, DeltaFileName)
                    if "not found" in base:
                        logmsg = "Patching failed on " + str(DeltaFileName) + ", base file not found"
                        psfxwkrlog.log(logging.ERROR, logmsg)
                        continue

                    status, PEbuf = MSDelta_imp.delta_patch(base, forward_delta)
                    if status == 1:
                        output = dest + "\\" + DeltaNameParts[0] + "\\" + DeltaFileName
                        with open(output, 'wb') as PE_out:
                            PE_out.write(PEbuf)
                    else:
                        logmsg = "Patching failed on " + str(DeltaFileName) + ", error code: " + str(status)
                        psfxwkrlog.log(logging.ERROR, logmsg)
                        continue
        else:
            cls.performcabextract("*.dll", src, dest, psfxwkrlog)
            cls.performcabextract("*.exe", src, dest, psfxwkrlog)
            cls.performcabextract("*.sys", src, dest, psfxwkrlog)
            
            for root, dirs, files in os.walk(dest):
                if "f" in dirs or "r" in dirs or "n" in dirs:
                    filesearch = root
                for file in files:
                    if "\\n\\" in root or root.endswith("\\n"):
                        # change dest to place that makes sense later
                        null = root + "\\" + file
                        output = filesearch + "\\" + file
                        status, error = MSDelta_imp.patch_binary(None, None, None, output, null)
                        if not status == 0:
                            # error handling code here
                            logmsg = "Patching failed on " + file + " with Windows Error Code: " + error
                            psfxwkrlog.log(logging.INFO, logmsg)
                    elif "\\r\\" in root or root.endswith("\\r"):
                        reverse = root + "\\" + file
                        forward = reverse.replace("\\r\\", "\\f\\")
                        output = filesearch + "\\" + file
                        
                        full_product_match = full_product_regex.search(filesearch)
                        product_name = chaff_regex.split(full_product_match[0])[0]
                        base = cls.findBaseFile(index, product_name, version, file)
                        if "not found" in base:
                            logmsg = "Patching failed on " + file + ", base file not Found"
                            psfxwkrlog.log(logging.INFO, logmsg)
                            continue
                        status, error = MSDelta_imp.patch_binary(base, forward, reverse, output)
                        if not status == 0:
                            # error handling code here
                            logmsg = "Patching failed on " + file + " with Windows Error Code: " + error
                            psfxwkrlog.log(logging.INFO, logmsg)
                    elif "\\f\\" in root or root.endswith("\\f"):
                        break

        # dest contains all the extracted content so just return that
        return (dest, UpdateSha)

    def cabdonesig(self):
        self.cabmgrRunning.clear()

    def cabstartsig(self):
        self.cabmgrRunning.set()

    def addjob(self, psfxcab: Tuple):
        # just add the cab to the queue.
        self.jobs.put(psfxcab)

    def passresult(self, future: asyncio.Future):
        fexception = future.exception()
        if fexception is KeyboardInterrupt:
            self.psfxmgrlogger.log(logging.INFO, "[PSFXMGR] execution stopped by user")
        elif fexception is not None:
            self.psfxmgrlogger.log(logging.ERROR, "[PSFXMGR] {-} exception occurred: " + str(fexception) + \
                "\n\ttraceback: " + tb.format_exc())
        else:
            result = future.result()
            self.pemgr.addjob(result[0], result[1])


class PEMgr(threading.Thread):
    '''
    after extraction is complete sorts through collected files and removes those that
    are already in the database. Finally passes remaining files to SymMgr for Symbol
    collection.
    poolsize - number of worker processes to spawn
    symmgr - SymbolMgr to pass results to
    db - DBMgr for which to refer database write jobs to
    '''
    def __init__(self, poolsize: int, symmgr, db, globqueue: queue.Queue):
        super(PEMgr, self).__init__()
        self.poolsize = poolsize
        self.symmgr = symmgr
        self.dbc = db
        self.globqueue = globqueue
        # jobs - holds the paths to the directories containing binaries to sort and
        # clean up
        self.jobs = queue.Queue()
        #  is an event to indicate that CabMgr has job items ready
        self.cabmgrRunning = threading.Event()
        self.psfxmgrRunning = threading.Event()
        self.userinterrupt = threading.Event()
        self.pemgrlogger = logging.getLogger("BAM.Pools.PEMgr")
         # self.daemon = True

    def addjob(self, pejobs: str, updateid: str):
        '''
        receive jobs from CabMgr, add those jobs to jobs List, and set
        the  Event to indicate there are jobs waiting to be
        processed
        '''
        item = (pejobs, updateid)
        self.jobs.put(item)

    def cabstartsig(self):
        self.cabmgrRunning.set()
    
    def cabdonesig(self):
        '''
        Used to notify that there are no more jobs coming in from CabMgr
        so the last batch of work should proceed
        '''
        self.cabmgrRunning.clear()
        
    def psfxstartsig(self):
        self.psfxmgrRunning.set()

    def psfxdonesig(self):
        self.psfxmgrRunning.clear()

    def passresult(self, future: asyncio.Future):
        '''
        takes a future generated by the executor and passes result over to
        SymbolMgr for processing. Since result is guaranteed to be new to DB
        at this point, also submits result to DBMgr to update DB
        '''
        fexception = future.exception()
        if fexception is KeyboardInterrupt:
            self.pemgrlogger.log(logging.INFO, "[PEMGR] execution stopped by user")
        elif fexception is not None:
            self.pemgrlogger.log(logging.ERROR, "[PEMGR] {-} exception occurred: " + str(fexception) + \
            "\n\ttraceback: " + tb.format_exc())
        else:
            result = future.result()
            if result is not None:
                if result[3]["builtwithdbginfo"] and self.symmgr is not None:
                    # only need to use verify if PE was builtwithdbginfo here because 
                    # that condition takes precedence over the stripped condition and 
                    # if the item is stripped, a check must be made by symchk anyway 
                    # to find the .dbg file.
                    # Reference:
                    # https://docs.microsoft.com/en-us/windows-hardware/drivers
                    # /debugger/symchk-command-line-options
                    # in the DBG file options.
                    jobitem = (str(result[0][0]), result[1], result[2], result[0][1])
                    self.symmgr.addjob(jobitem)
                    self.pemgrlogger.log(logging.INFO, "[PEMGR] items passed to symmgr: " + str(result[0][0]))

                self.dbc.addtask("binary", result[0], result[1], result[2], result[3])

    def run(self):
        '''
        spawns, manages, and tasks workers to perform pe functionalities
        '''
        self.cabmgrRunning.wait()
        self.psfxmgrRunning.wait()
        self.pemgrlogger.log(logging.INFO, "[PEMGR] PEMgr starting")
        start_time = time()
        if self.symmgr is not None:
            self.symmgr.pestartsig()

        # setup workers and Executor
        global _mgrlogger
        futures = []
        with ProcessPoolExecutor(max_workers=self.poolsize, initializer=wkr_logconfig, initargs=(self.globqueue, _mgrlogger)) as executor:
            while not self.jobs.empty() or self.psfxmgrRunning.is_set() or self.cabmgrRunning.is_set():
                if self.userinterrupt.is_set():
                    self.pemgrlogger.log(logging.INFO, "[PEMGR] execution stopped by user")
                    break
                try:
                    item = self.jobs.get(block=True, timeout=60)
                    jobdir = item[0]
                    updateid = item[1]
                except queue.Empty:
                    # this is done because we know that we'll get instances of queue being empty
                    # but since we may still want to wait for other items to come in while the 
                    # previous manager is running, we need to ignore the queue being empty.
                    # Previously we used the regular get which blocked for waiting for another 
                    # item, but this ended up causing the thread to hang because the loop could
                    # not check the event that said the previous manager had completed.
                    continue
                for root, _, files in os.walk(jobdir):
                    for file in files:
                        filepath = str(Path(os.path.join(root, file)).resolve())
                        self.pemgrlogger.log(logging.INFO, "[PEMGR] assigning pe job for " + str(filepath))
                        
                        # file naming convention in wsuscontent folder seems to be the same
                        # as whatever is at the end of update file obtained from update catalog
                        # so for our current purposes, searching via this regex is fine, but
                        # this format doesn't match with the UpdateID column in the database
                        # so correlation will have to be done in a different way?
                        # Using sha256 hash of file name for now, but need to figure something
                        # else out eventually since we may want to be able to correlate
                        # to the WSUS DB eventually. That would mean I have to find out how to
                        # get the real Update ID from somewhere.
                        bytes = str(filepath).encode('utf-8')
                        # updateid = hashlib.sha256(bytes).hexdigest()

                        future = executor.submit(self.petask, str(filepath), updateid)
                        future.add_done_callback(self.passresult)
                        futures.append(future)
            wait(futures)                 
        
        self.pemgrlogger.log(logging.INFO, "[PEMGR] *************part 2 done*****************")
        print("[PEMGR] **************part 2 done**********************")

        if self.symmgr is not None:
            self.symmgr.pedonesig()
            self.pemgrlogger.log(logging.INFO, "[PEMGR] done signal sent to symmgr")
        else:
            self.pemgrlogger.log(logging.INFO, "[PEMGR] symmgr doesn't exist because working on update mode only")
        self.dbc.donesig()

        endtime = time()
        elapsedtime = endtime-start_time
        print("elapsed time for part 2: " + str(elapsedtime))

    @classmethod
    def petask(cls, jobfile: str, updateid: str) -> Tuple | None:
        '''
        task to process PEs before submitting jobs for symbol search
        if item is removed, None is returned, else return item
        '''
        clnlogger = logging.getLogger("BAM.Pools.ClnWkr")
        
        results = None

        logmsg = "[PEMGR] Starting on " + str(jobfile)
        clnlogger.log(logging.INFO, logmsg)

        if ispe(jobfile):
            # check db to see if job already exists:
            hashes = getfilehashes(jobfile)

            if hashes is None:
                return None

            if wsuse_db.dbentryexistwithsymbols(globs.DBCONN.cursor(),     \
                                globs.PATCHEDFILESDBNAME, hashes[0], hashes[1]):
                # if PE is already in db with symbols obtained,
                # do not retask job to symbol manager, return None instead
                return results
            else:
                pass
                logmsg = "[PEMGR] continuing forward with " + str(jobfile)
                clnlogger.log(logging.INFO, logmsg)

            # getting to this point means item is not in db, may need to come up
            # with case where db needs to update item though
            infolist = {
                    'OriginalFilename': '', 'FileDescription': '', 'ProductName': '',
                    'Comments': '', 'CompanyName': '', 'FileVersion': '',
                    'ProductVersion': '', 'IsDebug': '', 'IsPatched': '',
                    'IsPreReleased': '', 'IsPrivateBuild': '', 'IsSpecialBuild': '',
                    'Language': '', 'PrivateBuild': '', 'SpecialBuild': ''
                    }

            try:
                unpefile = pefile.PE(jobfile, fast_load=True)
            except pefile.PEFormatError as peerror:
                logmsg = "[WSUS_DB] skipping " + str(jobfile) + " due to exception: " + peerror.value
                clnlogger.log(logging.ERROR, logmsg)
                return results

            infolist['fileext'], infolist['stype'] = pebinarytype(unpefile)
            infolist['arch'] = getpearch(unpefile)
            infolist['age'] = getpeage(unpefile)
            infolist['strippedpe'] = ispedbgstripped(unpefile)
            infolist['builtwithdbginfo'] = ispebuiltwithdebug(unpefile)            

            direntires=[ pefile.DIRECTORY_ENTRY['IMAGE_DIRECTORY_ENTRY_DEBUG'],    \
                pefile.DIRECTORY_ENTRY['IMAGE_DIRECTORY_ENTRY_RESOURCE'] ]
            unpefile.parse_data_directories(directories=direntires)
            infolist['pdbfilename'] = getpepdbfilename(unpefile)
            infolist['signature'] = getpesigwoage(unpefile)

            # a PE only have 1 VERSIONINFO, but multiple language strings
            # More information on different properites can be found at
            # https://msdn.microsoft.com/en-us/library/windows/desktop/aa381058
            # https://msdn.microsoft.com/en-us/library/windows/desktop/aa381049
            if getattr(unpefile, "VS_VERSIONINFO", None) is not None and \
                getattr(unpefile, "FileInfo", None) is not None:
                    for fileinfoentries in unpefile.FileInfo:
                        for fileinfoentry in fileinfoentries:
                            if getattr(fileinfoentry, "StringTable", None) is not None:
                                for strtable in fileinfoentry.StringTable:
                                    # Currently only handling unicode en-us
                                    if strtable.LangID[:4] == b'0409' or \
                                            (strtable.LangID[:4] == b'0000' and
                                            (strtable.LangID[4:] == b'04b0' or
                                            strtable.LangID[4:] == b'04B0')):
                                        infolist["Language"] \
                                            = strtable.LangID.decode("utf-8")
                                        for field, value in strtable.entries.items():
                                            dfield = field.decode('utf-8')
                                            dvalue = value.decode('utf-8')
                                            if dfield == "OriginalFilename":
                                                infolist["OriginalFilename"] \
                                                    = dvalue
                                            if dfield == "FileDescription":
                                                infolist["FileDescription"] \
                                                    = dvalue
                                            if dfield == "ProductName":
                                                infolist["ProductName"] \
                                                    = dvalue
                                            if dfield == "Comments":
                                                infolist["Comments"] \
                                                    = dvalue
                                            if dfield == "CompanyName":
                                                infolist["CompanyName"] \
                                                    = dvalue
                                            if dfield == "FileVersion":
                                                infolist["FileVersion"] \
                                                    = dvalue
                                            if dfield == "ProductVersion":
                                                infolist["ProductVersion"] \
                                                    = dvalue
                                            if dfield == "IsDebug":
                                                infolist["IsDebug"] \
                                                    = dvalue
                                            if dfield == "IsPatched":
                                                infolist["IsPatched"] \
                                                    = dvalue
                                            if dfield == "IsPreReleased":
                                                infolist["IsPreReleased"] \
                                                    = dvalue
                                            if dfield == "IsPrivateBuild":
                                                infolist["IsPrivateBuild"] \
                                                    = dvalue
                                            if dfield == "IsSpecialBuild":
                                                infolist["IsSpecialBuild"] \
                                                    = dvalue
                                            if dfield == "PrivateBuild":
                                                infolist["PrivateBuild"] \
                                                    = dvalue
                                            if dfield == "SpecialBuild":
                                                infolist["SpecialBuild"] \
                                                    = dvalue
            # Get the OS this PE is designed towards.
            # Microsoft PE files distributed via Microsoft's Updates typically
            # use the ProductVersion file properties to indicate the OS a specific
            # PE file is built towards.
            # If this is a Microsoft binary, the Product version is typically
            # the OS version it was built towards, but other products this is not
            # necessarily true
            if infolist['ProductName'].find("Operating System") != -1:
                infolist['osver'] = "NT" + infolist['ProductVersion']
            else:
                infolist['osver'] = "UNKNOWN"

            unpefile.close()

            results = ((str(jobfile), updateid), hashes[0], hashes[1], infolist)
        else:
            # if jobfile is not a PE, then check if it's a cab. If not a cab, remove it.
            if not validatecab(str(jobfile)):
                logmsg = "[PEMGR] petask: Removing " + str(jobfile)
                clnlogger.log(logging.INFO, logmsg)

                rmfile(jobfile)

                logmsg = "[PEMGR] " + str(jobfile) + " removed, not PE or cab file"
                clnlogger.log(logging.INFO, logmsg)
            else:
                pass
                logmsg = "[PEMGR] " + str(jobfile) + " is nested cab, skipping"
                clnlogger.log(logging.INFO, logmsg)
            return results

        logmsg = "[PEMGR] completed one petask for " + str(jobfile)
        clnlogger.log(logging.INFO, logmsg)

        return results


class SymMgr(threading.Thread):
    '''
    poolsize - number of worker processes to spawn
    symserver - Symbol Server with which to refer to when using symchk
    symdest - destination folder for symbols if downloading new symbols
    dbc - DBMgr for which to refer database write jobs to
    '''
    def __init__(self, poolsize: int, symserver: str, symdest: str, db, symlocal:bool=False, globqueue:queue.Queue=None):
        super(SymMgr, self).__init__()
        self.poolsize = poolsize
        self.symserver = symserver
        self.symdest = symdest
        self.dbc = db
        self.symlocal = symlocal
        self.globqueue = globqueue
        # jobs - queue of files to search for symbols for
        self.jobs = queue.Queue()
        #  - an Event that indicates to SymMgr when jobs are ready
        # for it to process
        self.peRunning = threading.Event()
        self.userinterrupt = threading.Event()
        self.symmgrlogger = logging.getLogger("BAM.Pools.SymMgr")
         # self.daemon = True

    def pestartsig(self):
        '''
        indicates that PEMgr is running so continue to run even if queue is
        empty since more stuff could come down the pipeline
        '''
        self.peRunning.set()

    def pedonesig(self):
        '''
        Used to notify that there are no more jobs coming in from PEMgr
        so the last batch of work should proceed
        '''
        self.peRunning.clear()

    def addjob(self, jobset: Tuple):
        '''
        receive jobs from PEMgr, add those jobs to jobs List, and set
        the  Event to indicate there are jobs waiting to be
        processed
        '''
        if jobset is not None:
            self.jobs.put(jobset)

    def makedbrequest(self, future: asyncio.Future):
        '''
        once Symbols found, submit task to DBMgr to update database with found symbols
        '''
        fexception = future.exception()
        if fexception is KeyboardInterrupt:
            self.symmgrlogger.log(logging.INFO, "[SYMMGR] execution stopped by user")
        elif fexception is not None:
            self.symmgrlogger.log(logging.INFO, "[SYMMGR] {-} exception occurred: " + str(fexception) + "\ntraceback: " + \
                tb.format_exc())
        else:
            results = future.result()
            if results is not None:
                jobtuple = results[0]
                sha256 = results[1]
                sha1 = results[2]
                infolist = results[3]
                self.dbc.addtask("symbol", jobtuple, sha256, sha1, infolist)
            else:
                self.symmgrlogger.log(logging.INFO, "[SYMMGR] no symbols found")

    def run(self):
        '''
        spawns, manages, and tasks workers to perform pe functionalities
        '''
        self.peRunning.wait()
        start_time = time()
        # setup workers and Executor
        global _mgrlogger
        futures = []
        with ProcessPoolExecutor(max_workers=self.poolsize, initializer=wkr_logconfig, initargs=(self.globqueue, _mgrlogger)) as executor:
                # take item from jobs and assign to workers
            while not self.jobs.empty() or self.peRunning.is_set():
                if self.userinterrupt.is_set():
                    self.symmgrlogger.log(logging.INFO, "[SYMMGR] execution stopped by user")
                    break
                try:
                    job = self.jobs.get(block=True, timeout=60)
                except queue.Empty:
                    # this is done because we know that we'll get instances of queue being empty
                    # but since we may still want to wait for other items to come in while the 
                    # previous manager is running, we need to ignore the queue being empty.
                    # Previously we used the regular get which blocked for waiting for another 
                    # item, but this ended up causing the thread to hang because the loop could
                    # not check the event that said the previous manager had completed.
                    continue
                self.symmgrlogger.log(logging.INFO, "[SYMMGR] assigning symbol job")
                future = executor.submit(self.symtask, job, self.symserver, \
                    self.symdest, self.symlocal)
                future.add_done_callback(self.makedbrequest)
                futures.append(future)
            wait(futures)
                 
            print("[SYMMGR] job queue empty and previous manager complete, shutting down SYMMgr.")

        self.symmgrlogger.log(logging.INFO, "[SYMMGR] *************part 3 done*****************")
        print("[SYMMGR] **************part 3 done**********************")

        self.dbc.donesig()

        endtime = time()
        elapsedtime = endtime-start_time
        print("elapsed time for part 3: " + str(elapsedtime))

    @classmethod
    def symtask(cls, jobitem: Tuple, symserver: str, symdest: str, symlocal: bool) -> Tuple | None:
        '''
        perform symbol search for symbols of jobfile. If there are no symbols or symbols found
        are already in db, discard results. Else, return found symbols
        '''
        symlogger = logging.getLogger("BAM.Pools.SymWkr")
        jobfile = jobitem[0]
        PEhashes = (jobitem[1], jobitem[2])
        UpdateSha = jobitem[3]

        result = None
        logmsg = "[SYMMGR].. Getting SYM for (" + str(jobfile) + ")"
        symlogger.log(logging.INFO, logmsg)
        servers = ""

        if symlocal:
            servers = " \""+ symdest + "\""
        else:
            servers = "u \"srv*" + symdest + "*" + symserver +"\""

        args = (".\\tools\\x64\\symchk.exe /v \"" + str(jobfile) + "\" /s" + servers + " /od")

        try:
            with subprocess.Popen(args, shell=False, stdout=subprocess.PIPE, stderr=subprocess.PIPE) \
                    as psymchk:
                # communicate used as symchk's output is for one file and
                # is not "large or unlimited"
                pstdout, pstderr = psymchk.communicate()
                stdoutsplit = str(pstdout.decode("ascii")).split("\r\n")
                stderrsplit = str(pstderr.decode("ascii")).split("\r\n")

                logmsg = "[SYMMGR] Attempt to obtain symbols for " + str(jobfile) + " complete"
                symlogger.log(logging.INFO, logmsg)

                infolist = {}
                try:
                    unpefile = pefile.PE(jobfile)
                except pefile.PEFormatError as peerror:
                    logmsg = "[WSUS_DB] Caught: PE error " + str(peerror) + ". File: " + jobfile
                    symlogger.log(logging.ERROR, logmsg)
                    return result

                infolist['signature'] = getpesigwoage(unpefile)
                infolist['arch'] = getpearch(unpefile)
                infolist['UpdateSHA'] = UpdateSha

                unpefile.close()

                stderrsplit.append(symserver)
                result = ((str(jobfile), stderrsplit, stdoutsplit), PEhashes[0], PEhashes[1], infolist)
        except subprocess.CalledProcessError as error:
            logmsg = "[SYMMGR] {-} symchk failed with error: " + str(error) + ". File: " + jobfile
            symlogger.log(logging.ERROR, logmsg)
            result = None
        except FileNotFoundError as error:
            logmsg = ("[SYMMGR] {-} symchk.exe not found")
            symlogger.log(logging.ERROR, logmsg)
            result = None

        logmsg = "[SYMMGR] completed symtask for " + str(jobfile)
        symlogger.log(logging.INFO, logmsg)
        return result


class DBMgr(threading.Thread):
    '''
    handles all write transactions to db. other managers needing to write to db will
    submit request to DBmgr which will accept and perform transaction to prevent race
    conditions.
    dbConn - sqlite3 connection to database
    '''
    def __init__(self, exdest: str, dbconn:sqlite3.Connection=None):
        super(DBMgr, self).__init__()
        self.dbconn = dbconn
        # jobqueue - queue to hold database write tasks
        self.jobqueue = queue.Queue()
        # jobsig - event that indicates when there are jobs that are waiting to be processed
        self.jobsig = threading.Event()
        self.userinterrupt = threading.Event()
        # donecount - used to indicate when there are no more jobs being submit to the DBMgr
        # by any of the other 3 Mgrs. When count is 3, indicates that there are no more jobs
        self.donecount = 0
        self.dbrecordscnt = 0
        self.exdest = exdest
        self.dblogger = logging.getLogger("BAM.Pools.DbMgr")
         # self.daemon = True

    def addtask(self, optype: str, jobtuple: Tuple, sha256: str, sha1: str, infolist: dict[str,str]):
        '''
        takes results generated from other Mgrs and creates a task which is placed
        in the Queue
        '''
        task = (optype, jobtuple, sha256, sha1, infolist)
        self.jobqueue.put(task)
        self.dblogger.log(logging.INFO, "[DBMGR] " + optype + " task added to queue. Queue at " + \
               str(self.jobqueue.qsize()) + " tasks.")
        self.jobsig.set()

    def writeupdate(self, file: str, sha256: str, sha1: str):
        '''
        performs writes to DB for Update files
        should use function in wsuse_db
        '''
        self.dblogger.log(logging.INFO, "[DBMGR] writing update for (" + str(file) + ")")
        wsuse_db.writeupdate(file, sha256, sha1, conn=self.dbconn)

    def writebinary(self, file: str, updateid: str, sha256: str, sha1: str, infolist: dict[str,str]):
        '''
        performs write updates to db for Binary files
        should use function in wsuse_db
        '''
        self.dblogger.log(logging.INFO, "[DBMGR] writing binary for (" + str(file) + ")")
        wsuse_db.writebinary(file, updateid, sha256, sha1, infolist, conn=self.dbconn)

    def writesym(self, file: str, symchkerr: str, symchkout: str, sha256: str, sha1: str, infolist: dict[str,str]):
        '''
        performs write updates to db for Symbol Files
        should use function in wsuse_db
        '''
        self.dblogger.log(logging.INFO, "[DBMGR] writing symbol for (" + str(file) + ")")

        wsuse_db.writesymbol(file, symchkerr, symchkout, sha256, sha1, infolist, self.exdest, conn=self.dbconn)

    def donesig(self):
        '''
        When all other Mgrs are done working, set jobsig so DBMgr can handle
        rest of items in queue and complete
        '''
        self.donecount += 1
        if self.donecount >= 3:
            # set the flag so that the rest of the queue can be emptied out.
            self.jobsig.set()

    def run(self):
        '''
        handles requests from other Mgrs to write to DB
        '''
        self.dblogger.log(logging.INFO, "[DBMGR] DBMgr starting")
        start_time = time()
        in_transaction = False

        while self.donecount < 3:
            if self.userinterrupt.is_set():
                self.dblogger.log(logging.INFO, "[DBMGR] execution stopped by user.")
                self.dbconn.interrupt()
                break
            if self.jobqueue.empty():
                self.dblogger.log(logging.INFO, "[DBMGR] waiting for more database jobs")
                self.jobsig.wait()

            # once signal received, take tasks off queue and process
            while not self.jobqueue.empty():

                # For every 5k queries, end transaction, commit, then restart
                # transaction
                self.dblogger.log(logging.INFO, "[DBMGR][DBUP] " + str(self.dbrecordscnt) + " records ready...")

                if self.dbrecordscnt == 0 and not in_transaction:
                    # this case is only run once
                    print("[DBUP] Restarting limit count")
                    wsuse_db.starttransaction(self.dbconn)
                    in_transaction = True
                elif self.dbrecordscnt >= 5000 and in_transaction:
                    print("[DBUP] limit to commit hit")
                    wsuse_db.endtransaction(self.dbconn)
                    self.dbrecordscnt = 0
                    in_transaction = False

                task = self.jobqueue.get()
                self.dblogger.log(logging.INFO, "[DBMGR] assigning database job: " + str(task[0]))

                if task[0] == "update":
                    self.dbrecordscnt += 1
                    self.writeupdate(task[1][0], task[2], task[3])
                elif task[0] == "binary":
                    self.dbrecordscnt += 1
                    self.writebinary(task[1][0], task[1][1], task[2], task[3], task[4])
                elif task[0] == "symbol":
                    self.dbrecordscnt += 1
                    self.writesym(task[1][0], task[1][1], task[1][2], task[2], task[3], task[4])
                else:
                    self.dblogger.log(logging.INFO, "[DBMGR] task unrecognized")

                self.dblogger.log(logging.INFO, "[DBMGR] task done, queue at " + str(self.jobqueue.qsize()) + " tasks")

                if self.jobsig.is_set():
                    self.jobsig.clear()

        if in_transaction:
            wsuse_db.endtransaction(self.dbconn, True)
            in_transaction = False
        endtime = time()
        elapsedtime = endtime-start_time
        print("elapsed time for part 4: " + str(elapsedtime))
        self.dblogger.log(logging.INFO, "[DBMGR] ****************everything done********************")
        print("[DMMGR] **************everything done**********************")
